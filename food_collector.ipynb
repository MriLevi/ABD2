{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABD2\n",
    "\n",
    "Wij gaan tijdens deze opdracht een food-collector simulatie bouwen.\n",
    "In deze simulatie is het doel van de agents om zo veel mogelijk groen eten op te eten, terwijl ze rood eten vermijden.\n",
    "We gaan deze simulatie grid-based maken. \n",
    "\n",
    "We maken gebruik van de gymnasium API: https://gymnasium.farama.org/\n",
    "\n",
    "Ter visualisatie gebruiken we pygame.\n",
    "\n",
    "\n",
    "### Simulation properties:\n",
    "We beginnen in principe met een grid van 128x128, maar dit kan uitgebreid worden. \n",
    "We zullen de implementatie zoveel mogelijk scalable en aanpasbaar implementeren, door hyperparameters aan te maken voor alle belangrijke properties.\n",
    "\n",
    "- Number of agents\n",
    "- Grid size (default 128x128)\n",
    "- Good food to total square ratio\n",
    "- Good food spawning pattern (maybe)\n",
    "- Bad food to total square ratio\n",
    "- Bad food spawning pattern (maybe)\n",
    "- Episode duration\n",
    "\n",
    "### Agent properties:\n",
    "Actions: up, down, left, right, wait\n",
    "Perception: full information (knows coordinates of other agents, good food, and bad food)\n",
    "Agent rules: not yet specified\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.3 (SDL 2.0.22, Python 3.9.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pygame\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import random\n",
    "import time\n",
    "import tcod\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "from batch_simulation import AuctionBatchSimulation, NoAuctionSharedMemoryBatchSimulation, NoAuctionNoSharedMemoryBatchSimulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent():\n",
    "    def __init__(self, vision_range, grid_size):\n",
    "        self.location = [None, None]\n",
    "        self.points = 0\n",
    "        self.vision_range = vision_range\n",
    "        self.memory = []\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([0, 0]),\n",
    "            1: np.array([1, 0]),\n",
    "            2: np.array([0, 1]),\n",
    "            3: np.array([-1, 0]),\n",
    "            4: np.array([0, -1])\n",
    "        }\n",
    "        self.path = []\n",
    "        self.grid_size = grid_size\n",
    "        self.shared_observation = {}\n",
    "\n",
    "    def move(self, direction):\n",
    "        \"\"\"move an agent into a specific direction\"\"\"\n",
    "\n",
    "        new_location = self.location + direction\n",
    "        self.location = list(new_location)\n",
    "\n",
    "    def clip_vision(self, obs):\n",
    "        \"\"\"clip the observation of the agent to the vision_range\"\"\"\n",
    "\n",
    "        clipped_obs = {}\n",
    "\n",
    "        for k in obs.keys():\n",
    "            clipped_obs[k] = [x for x in obs[k]\n",
    "                              if abs(self.location[0] - x[0]) <= self.vision_range\n",
    "                              and abs(self.location[1] - x[1]) <= self.vision_range\n",
    "                              ]\n",
    "\n",
    "        return clipped_obs\n",
    "\n",
    "    def reachable_food_filter(self, food_locations, agent_locations):\n",
    "        \"\"\"filters available food_locations based on whether a different agent is closer to the food (currently based on global distance)\"\"\"\n",
    "        agent_location = [self.vision_range, self.vision_range]\n",
    "\n",
    "        filtered_food_locations = []\n",
    "\n",
    "        for fl in food_locations:\n",
    "            unavailable = False\n",
    "            self_agent_distance = abs(agent_location[0] - fl[0]) + abs(agent_location[1] - fl[1])\n",
    "\n",
    "            other_agent_distances = [abs(al[0] - fl[0]) + abs(al[1] - fl[1]) for al in agent_locations]\n",
    "\n",
    "            for oad in other_agent_distances:\n",
    "                if oad < self_agent_distance:\n",
    "                    unavailable = True\n",
    "\n",
    "            if not unavailable:\n",
    "                filtered_food_locations.append(fl)\n",
    "\n",
    "        return filtered_food_locations\n",
    "\n",
    "    def create_cost_array(self, obs, cost_dict={\"default\": 2,\n",
    "                                                                            \"bad_food_locs\": 15,\n",
    "                                                                            \"good_food_locs\": 1,\n",
    "                                                                            \"agent_locs\": 0\n",
    "                                                                            }):\n",
    "        \"\"\"creates a 2d-array representing the cost of the different objects in the clipped agent observation\"\"\"\n",
    "\n",
    "        cost = np.full((self.grid_size, self.grid_size), cost_dict[\"default\"])\n",
    "\n",
    "        for k in obs.keys():\n",
    "            for coord in obs[k]:\n",
    "                cost[coord[0]][coord[1]] = cost_dict[k]\n",
    "        np.set_printoptions(threshold=sys.maxsize)\n",
    "        #print(np.rot90(cost, k=1, axes=(0, 1)))\n",
    "       \n",
    "        return cost\n",
    "    \n",
    "    def get_action_from_path(self, path):\n",
    "        \"\"\"Calculate what action to take based on the planned path\"\"\"\n",
    "        move = np.array(path[0]) - np.array(self.location)\n",
    "        return [list(x) for x in list(self._action_to_direction.values())].index(list(move))\n",
    "    \n",
    "    def get_path_to_target(self, target, cost):\n",
    "        \n",
    "        \"\"\"Calculate the path to a specific coordinate based on a cost-map\"\"\"\n",
    "        graph = tcod.path.SimpleGraph(cost=cost, cardinal=1, diagonal=0)\n",
    "        pf = tcod.path.Pathfinder(graph)\n",
    "        pf.add_root(self.location)\n",
    "        pf.resolve()\n",
    "    \n",
    "        path = pf.path_to(target)[1:].tolist()\n",
    "        \n",
    "\n",
    "        return path\n",
    "\n",
    "    def get_cost_to_target(self, target, obs):\n",
    "        \"\"\"Calculate the cost to a specific coordinate based on a cost-map\"\"\"\n",
    "        cost = self.create_cost_array(obs)\n",
    "        path = self.get_path_to_target(target, cost)\n",
    "        cost = sum([cost[x[0]][x[1]] for x in path])\n",
    "        return cost\n",
    "\n",
    "    \n",
    "    ###AGENT BEHAVIOUR###\n",
    "    def collect(self, obs, target):\n",
    "        \"\"\"Collect food at a specific coordinate\"\"\"\n",
    "        cost = self.create_cost_array(obs)\n",
    "        path = self.get_path_to_target(target, cost)\n",
    "        if len(path) == 0:\n",
    "            action = self.explore(obs)\n",
    "        else:\n",
    "            action = self.get_action_from_path(path)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def explore(self, obs):\n",
    "        \"\"\"Explore the environment\"\"\"\n",
    "        other_agent_locations = [x for x in obs['agent_locs'] if x != self.location]\n",
    "        viable_actions = list(self._action_to_direction.keys())\n",
    "\n",
    "        # Rule 1: agents can't move into other agents\n",
    "        viable_actions = [v for v in viable_actions if\n",
    "                          list(self.location + self._action_to_direction[v]) not in other_agent_locations]\n",
    "        # Rule 2: agents can't move out of bounds\n",
    "        viable_actions = [v for v in viable_actions\n",
    "                          if list(self.location + self._action_to_direction[v])[0] >= 0\n",
    "                          and list(self.location + self._action_to_direction[v])[1] >= 0\n",
    "                          and list(self.location + self._action_to_direction[v])[0] < self.grid_size\n",
    "                          and list(self.location + self._action_to_direction[v])[1] < self.grid_size]\n",
    "\n",
    "        #Rule 3: never step on bad food (while exploring)\n",
    "        viable_actions = [k for k, v in\n",
    "                          {v: list(self.location + self._action_to_direction[v]) for v in viable_actions}.items()\n",
    "                          if v not in obs['bad_food_locs']]\n",
    "\n",
    "        #Rule 4: prioritize undiscovered squares over discovered squares without good food in vision range\n",
    "        # checks the last 15 coordinates (15 has the lowest average steps for sim completion)\n",
    "        viable_actions_higher_prio = [k for k, v in {v: list(self.location + self._action_to_direction[v]) for v in\n",
    "                                                     viable_actions}.items()\n",
    "                                      if v not in [x['agent_location'] for x in self.memory[-15:] if\n",
    "                                                   len(x['agent_obs']['good_food_locs']) == 0]]\n",
    "\n",
    "        # unstuck the agent if it gets caught in a rare loop\n",
    "        action_history = [x['agent_action'] for x in self.memory][-10:]\n",
    "        if len(action_history) >= 10:\n",
    "            if len(list(set(action_history))) == 2:\n",
    "                if len(list(\n",
    "                        set([action_history[l] for l in range(len(action_history)) if l % 2 == 0]))) == 1 and len(\n",
    "                    list(set([action_history[l] for l in range(len(action_history)) if l % 2 == 1]))) == 1:\n",
    "                    return random.choice(viable_actions)\n",
    "\n",
    "        if len(viable_actions_higher_prio) > 0:\n",
    "            action = random.choice(viable_actions_higher_prio)\n",
    "        else:\n",
    "            action = random.choice(viable_actions)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def run(self, obs,target=[0,0]):\n",
    "     \n",
    "        # clip the observation to the agents vision\n",
    "        clipped_obs = self.clip_vision(obs)\n",
    "         \n",
    "        # decide which action to take based on the previous observation\n",
    "        if target:\n",
    "            action = self.collect(self.shared_observation, target)\n",
    "        else:\n",
    "            action = self.explore(self.shared_observation)\n",
    "\n",
    "        # append the new information to the agent's memory\n",
    "        self.memory.append({'agent_action': action, \"agent_obs\": clipped_obs, \"agent_location\": self.location})\n",
    "        \n",
    "        #clip the memory to make simulation faster\n",
    "        self.memory = self.memory[-15:]\n",
    "        \n",
    "        return action  #%% md\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class foodCollectorEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", None], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self, render_mode=None, grid_size=128, number_of_rounds=200, agents=1, good_food_ratio=0.1,\n",
    "                 bad_food_ratio=0.1, step_sleep=0, agent_vision_range = 5, random_seed = random.random()):\n",
    "        \n",
    "        self.random_seed = random_seed\n",
    "        self.step_sleep = step_sleep\n",
    "        self.grid_size = grid_size\n",
    "        self.window_size = 1028\n",
    "        self.game_duration = (number_of_rounds -1) * agents\n",
    "        self.current_step = 0\n",
    "        self.agents = [agent(agent_vision_range, grid_size) for _ in range(agents)]\n",
    "        self.good_food_ratio = good_food_ratio\n",
    "        self.bad_food_ratio = bad_food_ratio\n",
    "        self.good_food_points = 5\n",
    "        self.bad_food_points = -1\n",
    "        self.render_mode = render_mode\n",
    "        self.window = None\n",
    "        self.grid = np.ones((self.grid_size, self.grid_size))\n",
    "        self.nr_of_agents = agents\n",
    "\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'agent_locs': spaces.Sequence(spaces.MultiDiscrete([self.grid_size - 1, self.grid_size - 1])),\n",
    "            'good_food_locs': spaces.Sequence(spaces.MultiDiscrete([self.grid_size - 1, self.grid_size - 1])),\n",
    "            'bad_food_locs': spaces.Sequence(spaces.MultiDiscrete([self.grid_size - 1, self.grid_size - 1])),\n",
    "        })\n",
    "\n",
    "        self.action_space = spaces.MultiDiscrete(5)\n",
    "\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([0, 0]),\n",
    "            1: np.array([1, 0]),\n",
    "            2: np.array([0, 1]),\n",
    "            3: np.array([-1, 0]),\n",
    "            4: np.array([0, -1])\n",
    "        }\n",
    "\n",
    "        self.good_food_locations = []\n",
    "        self.bad_food_locations = []\n",
    "\n",
    "    def get_obs(self):\n",
    "        return {\n",
    "            \"agent_locs\": [agent.location for agent in self.agents],\n",
    "            \"good_food_locs\": self.good_food_locs,\n",
    "            \"bad_food_locs\": self.bad_food_locs\n",
    "        }\n",
    "    \n",
    "    def get_shared_observation(self):\n",
    "        observation = self.get_obs()\n",
    "        shared_observation = {'agent_locs': [], 'good_food_locs': [], 'bad_food_locs': []}\n",
    "\n",
    "        for agent in self.agents:\n",
    "            clipped_obs = agent.clip_vision(observation)\n",
    "            for key in shared_observation.keys():\n",
    "                for v in clipped_obs[key]:\n",
    "                    if v not in shared_observation[key]:\n",
    "                        shared_observation[key].append(v)\n",
    "\n",
    "        return shared_observation\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self.agent_locs = []\n",
    "        self.good_food_locs = []\n",
    "        self.bad_food_locs = []\n",
    "        \n",
    "        for agent in self.agents:\n",
    "            agent.memory = []\n",
    "            agent.points = 0\n",
    "        \n",
    "        \n",
    "        self.current_step = 0\n",
    "        #self.agents = [agent(agent_vision_range, grid_size) for _ in range(self.nr_of_agents)]\n",
    "\n",
    "\n",
    "        # place agents\n",
    "        random.seed(self.random_seed)\n",
    "        while True:\n",
    "            if len(self.agent_locs) >= len(self.agents):\n",
    "                break\n",
    "            loc = [random.randint(0, self.grid_size - 1), random.randint(0, self.grid_size - 1)]\n",
    "            if loc not in self.agent_locs:\n",
    "                self.agent_locs.append(loc)\n",
    "\n",
    "        # place good food\n",
    "        while True:\n",
    "            if len(self.good_food_locs) >= round(self.good_food_ratio * self.grid_size * self.grid_size):\n",
    "                break\n",
    "            loc = [random.randint(0, self.grid_size - 1), random.randint(0, self.grid_size - 1)]\n",
    "            if loc not in self.agent_locs and loc not in self.good_food_locs and loc not in self.bad_food_locs:\n",
    "                self.good_food_locs.append(loc)\n",
    "\n",
    "        #place bad food\n",
    "        while True:\n",
    "            if len(self.bad_food_locs) >= round(self.bad_food_ratio * self.grid_size * self.grid_size):\n",
    "                break\n",
    "            loc = [random.randint(0, self.grid_size - 1), random.randint(0, self.grid_size - 1)]\n",
    "            if loc not in self.agent_locs and loc not in self.good_food_locs and loc not in self.bad_food_locs:\n",
    "                self.bad_food_locs.append(loc)\n",
    "\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            agent.location = self.agent_locs[i]\n",
    "\n",
    "        observation = self.get_obs()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render_frame()\n",
    "\n",
    "        info = {\"agent_points\": [agent.points for agent in self.agents]}\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action, agent_index):\n",
    "\n",
    "        direction = self._action_to_direction[action[0]]\n",
    "\n",
    "        self.agents[agent_index].move(direction)\n",
    "\n",
    "        if self.agents[agent_index].location in self.good_food_locs:\n",
    "            self.good_food_locs.remove(self.agents[agent_index].location)\n",
    "            self.agents[agent_index].points += self.good_food_points\n",
    "\n",
    "        if self.agents[agent_index].location in self.bad_food_locs:\n",
    "            self.bad_food_locs.remove(self.agents[agent_index].location)\n",
    "            self.agents[agent_index].points += self.bad_food_points\n",
    "        \n",
    "        terminated = self.current_step >= self.game_duration or (\n",
    "                len(self.get_obs()['good_food_locs']) == 0 and self.current_step > 0)\n",
    "\n",
    "        reward = 0  # niet relevant voor nu\n",
    "        observation = self.get_obs()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render_frame()\n",
    "        info = {\"agent_points\": [agent.points for agent in self.agents]}\n",
    "\n",
    "        self.current_step += 1\n",
    "        time.sleep(self.step_sleep)\n",
    "\n",
    "        return observation, reward, terminated, info\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            return self.render_frame()\n",
    "\n",
    "    def render_frame(self):\n",
    "        pygame.font.init()\n",
    "        font = pygame.font.Font(None, 24)\n",
    "        scoreboard_width = 0.2\n",
    "\n",
    "        if self.window is None and self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode(\n",
    "                (self.window_size + self.window_size * scoreboard_width, self.window_size)\n",
    "            )\n",
    "\n",
    "        canvas = pygame.Surface((self.window_size + self.window_size * scoreboard_width, self.window_size))\n",
    "        canvas.fill((204, 255, 229))\n",
    "        pix_square_size = (\n",
    "                self.window_size / self.grid_size\n",
    "        )\n",
    "\n",
    "        #draw the background for the scoreboard\n",
    "        scoreboard_bg = pygame.Surface(\n",
    "            (self.window_size * scoreboard_width, self.window_size + self.window_size * scoreboard_width))\n",
    "        scoreboard_bg.fill((255, 255, 255))\n",
    "        scoreboard_bg.set_alpha(255)\n",
    "        canvas.blit(scoreboard_bg, (self.window_size, 0))\n",
    "\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            #draw agent picture\n",
    "            deer_image = pygame.image.load(\"deer.png\").convert_alpha()\n",
    "            deer_image = pygame.transform.scale(deer_image, (pix_square_size, pix_square_size))\n",
    "            canvas.blit(deer_image, (\n",
    "                agent.location[0] * pix_square_size, (self.grid_size - 1 - agent.location[1]) * pix_square_size))\n",
    "\n",
    "            #try to draw vision_range, a bit awkward imo\n",
    "            fov = pygame.Surface(\n",
    "                (pix_square_size * (agent.vision_range * 2 + 1), pix_square_size * (agent.vision_range * 2 + 1)))\n",
    "            fov.set_alpha(128)\n",
    "            fov.fill((137, 137, 137))\n",
    "            canvas.blit(fov, ((agent.location[0] * pix_square_size) - pix_square_size * agent.vision_range, ((\n",
    "                                                                                                                     self.grid_size - 1 -\n",
    "                                                                                                                     agent.location[\n",
    "                                                                                                                         1]) * pix_square_size) - pix_square_size * agent.vision_range))\n",
    "\n",
    "            score_text = font.render(f'Agent {i} | {agent.points}', True, \"black\")\n",
    "            canvas.blit(score_text, (self.window_size + 10, 10 + (i * 50)))\n",
    "\n",
    "            score_text = font.render(f\"{i}\", True, \"white\")\n",
    "            canvas.blit(score_text, (\n",
    "                agent.location[0] * pix_square_size, (self.grid_size - 1 - agent.location[1]) * pix_square_size))\n",
    "\n",
    "            #path to good food\n",
    "            if len(agent.path) > 1:\n",
    "                for coord in agent.path[1:]:\n",
    "                    #draw the path\n",
    "                    path = pygame.Surface((pix_square_size, pix_square_size))\n",
    "                    path.set_alpha(128)\n",
    "                    path.fill((255, 0, 0))\n",
    "                    canvas.blit(path, (coord[0] * pix_square_size, (self.grid_size - 1 - coord[1]) * pix_square_size))\n",
    "\n",
    "\n",
    "        for gfl in self.good_food_locs:\n",
    "            good_food_image = pygame.image.load(\"plant.png\").convert_alpha()\n",
    "            good_food_image = pygame.transform.scale(good_food_image, (pix_square_size, pix_square_size))\n",
    "            canvas.blit(good_food_image, (gfl[0] * pix_square_size, (self.grid_size - 1 - gfl[1]) * pix_square_size))\n",
    "\n",
    "        for bfl in self.bad_food_locs:\n",
    "            bad_food_image = pygame.image.load(\"evil_plant.png\").convert_alpha()\n",
    "            bad_food_image = pygame.transform.scale(bad_food_image, (pix_square_size, pix_square_size))\n",
    "            canvas.blit(bad_food_image, (bfl[0] * pix_square_size, (self.grid_size - 1 - bfl[1]) * pix_square_size))\n",
    "\n",
    "        for x in range(self.grid_size + 1):\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                color=0,\n",
    "                start_pos=(0, pix_square_size * x),\n",
    "                end_pos=(self.window_size, pix_square_size * x),\n",
    "                width=1,\n",
    "            )\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                color=0,\n",
    "                start_pos=(pix_square_size * x, 0),\n",
    "                end_pos=(pix_square_size * x, self.window_size),\n",
    "                width=1,\n",
    "            )\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "\n",
    "            #self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        else:\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('seeds.txt', 'r') as file:\n",
    "    random_seeds = [line.strip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = foodCollectorEnv(render_mode= 'human', grid_size=24, number_of_rounds=1000, agents=5, good_food_ratio=0.15,\n",
    "                       bad_food_ratio=0.25, step_sleep=0, agent_vision_range = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/10\n",
      "1/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m<timed exec>:4\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n",
      "File \u001B[1;32m~\\PycharmProjects\\ABD2\\batch_simulation.py:83\u001B[0m, in \u001B[0;36mAuctionBatchSimulation.run\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     79\u001B[0m shared_observation \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mget_shared_observation()\n\u001B[0;32m     81\u001B[0m good_food_locs \u001B[38;5;241m=\u001B[39m shared_observation[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgood_food_locs\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m---> 83\u001B[0m cost_dict \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     84\u001B[0m     agent_ind: {\u001B[38;5;28mstr\u001B[39m(k): \u001B[38;5;241m1000\u001B[39m \u001B[38;5;241m-\u001B[39m env\u001B[38;5;241m.\u001B[39magents[agent_ind]\u001B[38;5;241m.\u001B[39mget_cost_to_target(k, shared_observation) \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m\n\u001B[0;32m     85\u001B[0m                 good_food_locs} \u001B[38;5;28;01mfor\u001B[39;00m agent_ind \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(env\u001B[38;5;241m.\u001B[39magents))}\n\u001B[0;32m     86\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(env\u001B[38;5;241m.\u001B[39magents)):\n\u001B[0;32m     87\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(env\u001B[38;5;241m.\u001B[39magents)):\n",
      "File \u001B[1;32m~\\PycharmProjects\\ABD2\\batch_simulation.py:84\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     79\u001B[0m shared_observation \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mget_shared_observation()\n\u001B[0;32m     81\u001B[0m good_food_locs \u001B[38;5;241m=\u001B[39m shared_observation[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgood_food_locs\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     83\u001B[0m cost_dict \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m---> 84\u001B[0m     agent_ind: {\u001B[38;5;28mstr\u001B[39m(k): \u001B[38;5;241m1000\u001B[39m \u001B[38;5;241m-\u001B[39m env\u001B[38;5;241m.\u001B[39magents[agent_ind]\u001B[38;5;241m.\u001B[39mget_cost_to_target(k, shared_observation) \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m\n\u001B[0;32m     85\u001B[0m                 good_food_locs} \u001B[38;5;28;01mfor\u001B[39;00m agent_ind \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(env\u001B[38;5;241m.\u001B[39magents))}\n\u001B[0;32m     86\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(env\u001B[38;5;241m.\u001B[39magents)):\n\u001B[0;32m     87\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(env\u001B[38;5;241m.\u001B[39magents)):\n",
      "File \u001B[1;32m~\\PycharmProjects\\ABD2\\batch_simulation.py:84\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     79\u001B[0m shared_observation \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mget_shared_observation()\n\u001B[0;32m     81\u001B[0m good_food_locs \u001B[38;5;241m=\u001B[39m shared_observation[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgood_food_locs\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     83\u001B[0m cost_dict \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m---> 84\u001B[0m     agent_ind: {\u001B[38;5;28mstr\u001B[39m(k): \u001B[38;5;241m1000\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43magents\u001B[49m\u001B[43m[\u001B[49m\u001B[43magent_ind\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_cost_to_target\u001B[49m\u001B[43m(\u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshared_observation\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m\n\u001B[0;32m     85\u001B[0m                 good_food_locs} \u001B[38;5;28;01mfor\u001B[39;00m agent_ind \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(env\u001B[38;5;241m.\u001B[39magents))}\n\u001B[0;32m     86\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(env\u001B[38;5;241m.\u001B[39magents)):\n\u001B[0;32m     87\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(env\u001B[38;5;241m.\u001B[39magents)):\n",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36magent.get_cost_to_target\u001B[1;34m(self, target, obs)\u001B[0m\n\u001B[0;32m     94\u001B[0m \u001B[38;5;124;03m\"\"\"Calculate the cost to a specific coordinate based on a cost-map\"\"\"\u001B[39;00m\n\u001B[0;32m     95\u001B[0m cost \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_cost_array(obs)\n\u001B[1;32m---> 96\u001B[0m path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_path_to_target\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcost\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     97\u001B[0m cost \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m([cost[x[\u001B[38;5;241m0\u001B[39m]][x[\u001B[38;5;241m1\u001B[39m]] \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m path])\n\u001B[0;32m     98\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cost\n",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36magent.get_path_to_target\u001B[1;34m(self, target, cost)\u001B[0m\n\u001B[0;32m     84\u001B[0m pf \u001B[38;5;241m=\u001B[39m tcod\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mPathfinder(graph)\n\u001B[0;32m     85\u001B[0m pf\u001B[38;5;241m.\u001B[39madd_root(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlocation)\n\u001B[1;32m---> 86\u001B[0m \u001B[43mpf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresolve\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     88\u001B[0m path \u001B[38;5;241m=\u001B[39m pf\u001B[38;5;241m.\u001B[39mpath_to(target)[\u001B[38;5;241m1\u001B[39m:]\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[0;32m     91\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m path\n",
      "File \u001B[1;32m~\\PycharmProjects\\RIVM_models\\venv\\lib\\site-packages\\tcod\\path.py:1271\u001B[0m, in \u001B[0;36mPathfinder.resolve\u001B[1;34m(self, goal)\u001B[0m\n\u001B[0;32m   1269\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m   1270\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_heuristic(goal)\n\u001B[1;32m-> 1271\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_graph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_resolve\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\RIVM_models\\venv\\lib\\site-packages\\tcod\\path.py:1070\u001B[0m, in \u001B[0;36mSimpleGraph._resolve\u001B[1;34m(self, pathfinder)\u001B[0m\n\u001B[0;32m   1069\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_resolve\u001B[39m(\u001B[38;5;28mself\u001B[39m, pathfinder: Pathfinder) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1070\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_subgraph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_resolve\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpathfinder\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\RIVM_models\\venv\\lib\\site-packages\\tcod\\path.py:982\u001B[0m, in \u001B[0;36mCustomGraph._resolve\u001B[1;34m(self, pathfinder)\u001B[0m\n\u001B[0;32m    979\u001B[0m \u001B[38;5;124;03m\"\"\"Run the pathfinding algorithm for this graph.\"\"\"\u001B[39;00m\n\u001B[0;32m    980\u001B[0m rules, keep_alive \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compile_rules()\n\u001B[0;32m    981\u001B[0m _check(\n\u001B[1;32m--> 982\u001B[0m     \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath_compute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    983\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpathfinder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_frontier_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    984\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpathfinder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_distance_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    985\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpathfinder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_travel_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    986\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mrules\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    987\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrules\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    988\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpathfinder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_heuristic_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    989\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    990\u001B[0m )\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# list of random seeds need to be longer than the number of runs\n",
    "#AuctionBatchSimulation, NoAuctionSharedMemoryBatchSimulation, NoAuctionNoSharedMemoryBatchSimulation\n",
    "sim = AuctionBatchSimulation(env, runs = 10, random_seeds = random_seeds)\n",
    "sim.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sim.df.to_csv(\"AuctionBatchSimulation_v1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2min 17s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
